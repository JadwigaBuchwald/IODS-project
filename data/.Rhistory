# m1 = Marker M1 allele frequency = MAF of marker SNP, proxy for causal SNP
# dprime = Linkage disequilibrium (D-prime)
# alpha = User-defined type I error rate
# da = Dominance : additive QTL effects = set nodom=FALSE to use this
# nodom = No dominance = flag to ignore da and assume purely allelic model
# parent = Both parents genotyped = flag for whether parents are included
# sibr = Sibling correlation
# s = Sibship Size = 1 for unrelated individuals, 2 for sib pairs, etc.
# whichpower = which power calc you want: "overall", "between", "within"
# for more details see notes: http://pngu.mgh.harvard.edu/~purcell/gpc/#qtl_ins
qtl.assoc.power = function(n,vq,p,m1,dprime,alpha,da=0,nodom=TRUE,parent=FALSE,sibr=1,s=1,whichpower="overall") {
if(nodom) {
tdf = 1 # 1 degree of freedom without dominance
# allele count is assumed to be linear with effect, so the only thing
# that can vary is the slope
} else {
tdf = 2 # 2 degrees of freedom with dominance
# allele counts 0, 1, 2 vs. their effects are not collinear
}
# allele frequencies
q = 1 - p # major AF of causal SNP
m2 = 1 - m1 # major AF of marker SNP
# recover un-normalized linkage disequilibrium
# back-calculate regular D based on D' which is passed to this function
# see http://en.wikipedia.org/wiki/Linkage_disequilibrium#Definition
dmax = p*m2
if (q*m1 < dmax) {
dmax = q*m1
}
ld = dprime * dmax
# calculate haplotype frequencies
h00 = p*m1 + ld # based on D from above, get haplotype frequencies
h01 = p*m2 - ld
h10 = q*m1 - ld
h11 = q*m2 + ld
# genetic values
vtot = 1
a = sqrt( (vq*vtot) / ( (2*p*q)*(1+da*(q-p))**2 + (2*p*q*da)**2 ) )
d = da*a
# variance components
va = 2 * p * q * ( a + d*(q-p))**2
vd = (2*p*q*d)*(2*p*q*d)
vs = sibr*vtot - va/2 - vd/4
vn = (1-sibr)*vtot - va/2 - 3*vd/4
# attenuation factor due to incomplete LD for association test
phisq =         ((h00 - m1*p)**2/(m1*p))
phisq = phisq + ((h10 - m1*q)**2/(m1*q))
phisq = phisq + ((h01 - m2*p)**2/(m2*p))
phisq = phisq + ((h11 - m2*q)**2/(m2*q))
vam = phisq * va
vdm = phisq**2 * vd
vsm = vs + (((1-phisq)*va)/2) + (((1-phisq**2)*vd)/4)
vnm = vn + (((1-phisq)*va)/2) + ((3*(1-phisq**2)*vd)/4)
# compute expected non-centrality parameters
# ncp_b = between sibships
# ncp_w = within sibships
ncp_b = log(vnm + s*vsm + ((s+1)/2)*vam + ((s+3)/4)*vdm) -
log(vnm + s*vsm)
# take sample size into account
ncp_b = ncp_b * n
between.sibships.power = likelihood.ratio.test.power(alpha,tdf,ncp_b)
if (whichpower == "between") {
return(between.sibships.power)
} else {
stop(paste("invalid whichvalue parameter:",whichpower))
}
}
# a few tests
qtl.assoc.power(n=1486,vq=0.0450,p=.1,m1=.1,dprime=.8,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
qtl.assoc.power(n=1486,vq=0.0450,p=.1,m1=.1,dprime=0.8944272,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
ld
r2=ld^2/(p*(1-p)*m1*(1-m1))
p=0.1
m1=0.1
r2=ld^2/(p*(1-p)*m1*(1-m1))
r2
b=0.5
afreq=0.1
b^2*2*afreq*(1-afreq)
n=1486
pchisq(chisq.threshold,df=1,ncp=r2*b^2*2*n*afreq*(1-afreq),lower=FALSE)
b=0.3
afreq=0.03
b^2*2*afreq*(1-afreq)
n=1486
pchisq(chisq.threshold,df=1,ncp=r2*b^2*2*n*afreq*(1-afreq),lower=FALSE)
b^2*2*afreq*(1-afreq)
qtl.assoc.power(n=1486,vq=0.005238,p=.03,m1=.03,dprime=0.8944272,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
qtl.assoc.power(n=1486,vq=0.005238,p=.03,m1=.03,dprime=0.8944272,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=0,whichpower="between")
b=0.3
afreq=0.03
n=1486
pchisq(chisq.threshold,df=1,ncp=r2*b^2*2*n*afreq*(1-afreq),lower=FALSE)
r2=0.64
b^2*2*afreq*(1-afreq)
qtl.assoc.power(n=1502,vq=0.005238,p=.03,m1=.03,dprime=0.8,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
r2=0.64
n=1502
pchisq(chisq.threshold,df=1,ncp=r2*b^2*2*n*afreq*(1-afreq),lower=FALSE)
pchisq(chisq.threshold,df=1,ncp=54.48,lower=FALSE)
maf=c(0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4)
b=c(0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8)
p=maf
m1=maf
vq=NULL
for(i in 1:48){
vq[i]=b[i]^2*2*maf[i]*(1-maf[i])
}
power=NULL
for(i in 1:48){
power[i]=qtl.assoc.power(n=1486,vq=vq[i],p=p[i],m1=m1[i],dprime=0.8944272,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
}
power=power*100
power=round(power, digits=2)
results<-data.frame(maf,b,power)
results
n=1486
r2=0.8
maf=c(0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4)
b=c(0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8)
str(maf)
str(b)
p.threshold=5e-08
chisq.threshold=qchisq(p.threshold,df=1,lower=FALSE,ncp=0)
ncp=NULL
for(i in 1:48){
ncp[i]=r2*b[i]^2*2*n*maf[i]*(1-maf[i])
}
power=NULL
for(i in 1:48){
power[i]=pchisq(chisq.threshold,df=1,ncp[i],lower=FALSE)
}
power=power*100
power=round(power, digits=2)
results<-data.frame(maf,b,power)
results2
n=1486
r2=0.8
maf=c(0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4)
b=c(0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8)
str(maf)
str(b)
p.threshold=5e-08
chisq.threshold=qchisq(p.threshold,df=1,lower=FALSE,ncp=0)
ncp=NULL
for(i in 1:48){
ncp[i]=r2*b[i]^2*2*n*maf[i]*(1-maf[i])
}
power=NULL
for(i in 1:48){
power[i]=pchisq(chisq.threshold,df=1,ncp[i],lower=FALSE)
}
power=power*100
power=round(power, digits=2)
results2<-data.frame(maf,b,power)
results2
maf=c(0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4,0.01, 0.03, 0.05, 0.1, 0.2, 0.4)
b=c(0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.4,0.4,0.4,0.4,0.4,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.7,0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.8,0.8)
p=maf
m1=maf
vq=NULL
for(i in 1:48){
vq[i]=b[i]^2*2*maf[i]*(1-maf[i])
}
power=NULL
for(i in 1:48){
power[i]=qtl.assoc.power(n=1486,vq=vq[i],p=p[i],m1=m1[i],dprime=0.8944272,alpha=5e-08,sibr=.5,s=1,parent=FALSE,nodom=TRUE,whichpower="between")
}
power=power*100
power=round(power, digits=2)
results<-data.frame(maf,b,power)
results
results_CPG_R<-cbind(results,results2)
View(results_CPG_R)
results_CPG_R
1518-(78*2)
2*(78)/(1+0.26)
ESS=123.8095+1362
ESS
load("Z:/Hjelt/PhD/Kurssit/SMIPH/R/.RData")
install.packages("Epi")
install.packages("Epi")
install.packages("Epi")
setInternet2()
chooseCRANmirror()
setInternet2()
install.packages("Epi")
"C:\Program Files\RStudio\bin\rstudio.exe" http_proxy=http://host:port/
install.packages('Epi', dependencies=TRUE, repos='http://cran.rstudio.com/")
install.packages('Epi', dependencies=TRUE, repos="http://cran.rstudio.com/")
install.packages('Epi', dependencies=TRUE, repos="http://cran.rstudio.com/")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
options(download.file.method = "internal")
utils::setInternet2(TRUE)
install.packages("plyr")
install.packages("plyr")
utils::setInternet2(TRUE)
options(download.file.method = "internal")
install.packages("plyr")
install.packages("survival")
getURL("www.google.com")
if(download.file(url="http://www.r-project.org/index.html",
destfile=tempfile()) == 0) {
print("Yuhuu")
}
-0.7*0.7
-0.7*0.7+0.7*0.9
-0.7*0.7+0.7*0.3
-0.7*0.1+0.7*0.9
0.7*0.9+0.7*0.9
0.7*0.3+0.3*0.3
0.7*0.3+0.7*0.3
1.26/0.42
0.56/-0.28
1e-6
a<-1e-6
b<-0.000001
111/207
60*0.6
44210562-39046965
44210562-39546965
40185505-500000
43684957+500000
40185505-50000
43684957+50000
43710562+ 50 000
43710562+50000
44210562-43710562
500 000
500000
44210562-500000 #=43710562
39546965-50000 #=43710562
43710562+500000 #= 43760562
39546965-500000 #=39496965
4*3*3
112+84
95+850+74
(644+165)+74+95
(644+165)+117+95
117-11-32
30+11+165+644
30+11+2+74
95+850+74+2
common<-644+165
lfv<-117
rare<-95
0.02465+0.01374+0.04866
sqrt(0.02465+0.01374+0.04866)
12+0.5+3.5+2
38+11
49+12
225+236+232
install.packages("GenABEL")
models<-c("basic”,”additional”,”specific_a”,”specific_b”)
traits<-c("nmr_transf","cpd_transf")
for (m in models){
for(i in traits){
for(n in 1:22){
d0<-paste("/FinnTwin_”,m,”_",i,".chr",n,".Meta",sep="")
d1<-paste("/FTALCO_FTNICO_HRC_chr",n,".info",sep="")
}
}
}
models<-c("basic”,”additional”,”specific_a”,”specific_b”)
)
models <- c("basic","additional","specific_a","specific_b")
models <- c("basic","additional","specific_a","specific_b")
traits<-c("nmr_tranfs","cpd_transf")
models <- c("basic","additional","specific_a","specific_b")
traits<-c("nmr_tranfs","cpd_transf")
for (m in models){
for(i in traits){
for(n in 1:22){
d0<-paste("/FinnTwin_”,m,”_",i,".chr",n,".Meta",sep="")
d1<-paste("/FTALCO_FTNICO_HRC_chr",n,".info",sep="")
}
}
}
d0
models <- c("basic","additional","specific_a","specific_b")
traits<-c("nmr_tranfs","cpd_transf")
for (m in models){
for(i in traits){
for(n in 1:22){
d0<-paste("/FinnTwin_",m,"_",i,".chr",n,".Meta",sep="")
d1<-paste("/FTALCO_FTNICO_HRC_chr",n,".info",sep="")
}
}
}
d0
0.003/0.017
0.017/5
0.017/5
install.packages("rmarkdown")
library(rmarkdown)
getOption("repos")
install.packages("rmarkdown", contriburl = "http://cran.r-project.org/src/contrib", type = "source")
install.packages("rmarkdown", contriburl = "http://cran.r-project.org/src/contrib", type = "source")
library(rmarkdown)
install.packages("Diagrammer")
install.packages("DiagrammeR")
plot(cars)
summary(mtcars)
names(mtcars)
abline(lm(dist~speed), col="red")
plot(cars)
abline(lm(dist~speed, data=cars), col="red")
summary(lm(dist~speed, data=cars))
summary(lm(speed~dist, data=cars))
summary(mtcars)
summary(lm(mpg~cyl + disp + hp, data=mtcars))
summary(lm(cyl~mpg + disp + hp, data=mtcars))
summary(lm(mpg~cyl + disp + hp, data=mtcars))
summary(lm(cyl~mpg + disp + hp+ drat, data=mtcars))
t1<-(-1.22742-0)/0.79728
t1<-(-1.22742-0)/0.79728
t2<-(-0.063581-0)/0.041299
t1; t2;
knitr::opts_chunk$set(echo = TRUE)
FR<-read.table("C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/3_Consortium/DataPreparation/FR/FR_419_withAllVariables_prechecksdone.txt",sep="\t", header=T, dec=".")
YFS<-read.table("./DataCreatedOnTheWay/YFS714_Nmaximized_FinalForDescriptives.txt",sep="\t", header=T, dec=".")
names(YFS)
str(YFS)
FT<-read.table("C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/3_Consortium/DataPreparation/FT/FT453_P_descriptives_withRtn.txt",sep="\t", header=T, dec="") #Nyt N=439
names(FT)
str(FT)
FR<-read.table("C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/3_Consortium/DataPreparation/FR/FR_419_withAllVariables_prechecksdone.txt",sep="\t", header=T, dec=".")
names(FR)
str(FR)
FT<-read.table("C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/3_Consortium/DataPreparation/FT/FT453_P_descriptives_withRtn.txt",sep="\t", header=T, dec=".") #Nyt N=439
names(FT)
str(FT)
YFS<-read.table("C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/3_Consortium/DataPreparation/YFS/DataCreatedOnTheWay/YFS714_Nmaximized_FinalForDescriptives.txt",sep="\t", header=T, dec=".")
names(YFS)
str(YFS)
YFS$gender<-as.factor(YFS$sex)
levels(YFS$gender)<-c("Male","Female")
FR$gender<-as.factor(FR$sex)
levels(FR$gender)<-c("Male","Female")
qplot(NMR,CPD,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR,Packyears,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR,Cotplus3HC,col=gender,data=FR)+geom_smooth(method="lm")
library(ggplot2)
qplot(NMR,CPD,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR,Packyears,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR,Cotplus3HC,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR_rtn,CPD_rtn,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR_rtn,Packyears_rtn,col=gender,data=FR)+geom_smooth(method="lm")
qplot(NMR_rtn,Cotplus3HC_rtn,col=gender,data=FR)+geom_smooth(method="lm")
locreg=function(df,df.x,df.y,xname,yname){
dummy.df <<- df
dummy.x <<- df.x
dummy.y <<- df.y
plot<- ggplot(data=dummy.df, aes(x=dummy.x, dummy.y))+geom_point(color='gray') +
geom_smooth(method='loess', span=0.2) +
geom_smooth(method='loess', span=0.5, color='red')+
xlab(xname)+
ylab(yname)+
theme_bw()
#print(plot)
return(plot)
}
locreg(FR,FR$NMR,FR$Cotinine,"NMR","Cot")
locreg(FR,FR$NMR,FR$X3HC,"NMR","X3HC")
locreg(FR,FR$NMR,FR$Cotplus3HC,"NMR","Cotplus3HC")
locreg(FR,FR$NMR,FR$CPD,"NMR","CPD")
locreg(FR,FR$X3HC,FR$Cotinine,"X3HC","Cot")
locreg(FR,FR$CPD,FR$Cotinine,"CPD","Cot")
locreg(FR,FR$CPD,FR$Cotplus3HC,"CPD","Cotplus3HC")
EA991<-read.table("Z:/Hjelt/PROJECTS/EWAS/Educational/SelectedSamplesUpdated_15Nov2015.txt",sep="\t", header=T, dec=".")
EA501<-read.table("Z:/Hjelt/PROJECTS/EWAS/Educational/N501/SelectedSamples_OneCoTwin_13Jan2016.txt",sep="\t", header=T, dec=".")
View(EA501)
EA501_forAntti<-EA501[,1:15]
View(EA501_forAntti)
write.table(EA501_forAntti,"C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/EA501_forAntti.txt",quote=F,sep="\t",row.names = F)
write.table(EA501_forAntti,"C:/HY-Data/JADWIGAB/PhD_FIMM/PhD_Analysis/EA501_forAntti_comma.txt",quote=F,sep="\t",row.names = F, dec=",")
View(EA501)
View(EA501)
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/HY-Data/JADWIGAB/PhD_FIMM/NotesOutsideTheOffice/Courses/OpenData2017/For_IODS/IODS-project")
library(GGally)
library(ggplot2)
m <- glm(high_use ~ sex + address + Mjob + studytime, data = alc, family = "binomial")
m <- glm(high_use ~ sex + address + Mjob + studytime, data = alc, family = "binomial")
setwd("C:/HY-Data/JADWIGAB/PhD_FIMM/NotesOutsideTheOffice/Courses/OpenData2017/For_IODS/IODS-project/data")
alc<-read.table("./alc.txt",sep="\t", header=T, dec=".")
m <- glm(high_use ~ sex + address + Mjob + studytime, data = alc, family = "binomial")
summary(m)
OR <- exp(coef(m))
cI<- exp(confint(m))
cbind(OR, CI)
cI <- exp(confint(m))
cbind(OR, CI)
CI <- exp(confint(m))
cbind(OR, CI)
m <- glm(high_use ~ sex + studytime, data = alc, family = "binomial")
alc$high_use_prob<- predict(m, type = "response")
summary(alc$high_use_prob)
alc$high_use_prob<- predict(m, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
m2 <- glm(high_use ~ sex + studytime, data = alc, family = "binomial")
# predict() the probability of high_use
alc$high_use_prob<- predict(m2, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
#we go back and look at our original model for fun
# predict() the probability of high_use
alc$high_use_prob<- predict(m, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
names(alc)
m3 <- glm(high_use ~ sex + studytime + absences + failures + age, data = alc, family = "binomial")
m3 <- glm(high_use ~ sex + studytime + absences + failures + age, data = alc, family = "binomial")
alc$high_use_prob<- predict(m3, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
m3 <- glm(high_use ~ sex + studytime + absences , data = alc, family = "binomial")
alc$high_use_prob<- predict(m3, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
names(alc)
m3 <- glm(high_use ~ sex + studytime + absences , data = alc, family = "binomial")
alc$high_use_prob<- predict(m3, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
g <- ggplot(alc, aes(x = "high_use", y = "high_use_prob"))
g + geom_point()
g <- ggplot(alc, aes(x = "high_use", y = "high_use_prob", col="high_use_pred"))
g + geom_point()
#####Including absences as during the DataCamp exercises we noticed this was a good predictor
names(alc)
m3 <- glm(high_use ~ sex + studytime + absences , data = alc, family = "binomial")
alc$high_use_prob<- predict(m3, type = "response")
# use the probabilities to make a prediction of high_use
alc$high_use_pred <- alc$high_use_prob > 0.5
# see the last ten original classes, predicted probabilities, and class predictions
tail(alc[,c("high_use", "high_use_prob", "high_use_pred")])
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# seeing why we get so odd results
summary(alc$high_use_prob)
# access dplyr and ggplot2
library(dplyr); library(ggplot2)
# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = "high_use", y = "high_use_prob", col="high_use_pred"))
# define the geom as points and draw the plot
g + geom_point()
table(high_use = alc$high_use, prediction = alc$high_use_pred)
summary(alc$high_use_prob)
g <- ggplot(alc, aes(x = "high_use_prob", y = "high_use", col="high_use_pred"))
g + geom_point()
g <- ggplot(alc, aes(x = high_use_prob, y = high_use, col=high_use_pred))
g + geom_point()
table(high_use = alc$high_use, prediction = alc$prediction)
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$high_use_pred)
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$high_use_pred)
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$high_use_pred)
(10+88)/382
loss_func(class = alc$high_use, prob = 0) #comparing to simply guessing none are high users.
